{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56abab67",
      "metadata": {
        "id": "56abab67"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "#  correct file path and extension\n",
        "file_path = \"updated_data.json\"\n",
        "\n",
        "# Read the JSON file into a DataFrame\n",
        "df = pd.read_json(file_path, lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d30968",
      "metadata": {
        "id": "52d30968",
        "outputId": "8ba267dc-46eb-443d-8ef1-35371a4c6c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb31c47",
      "metadata": {
        "id": "1fb31c47"
      },
      "outputs": [],
      "source": [
        "#a smaller, random sample (1% of the original) from the DataFrame\n",
        "df_sampled = df.sample(frac=0.002, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ff9c5d",
      "metadata": {
        "id": "07ff9c5d",
        "outputId": "174d3735-7476-44b4-fe3b-1b643399c417"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6372, 6)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sampled.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518ccdb6",
      "metadata": {
        "scrolled": false,
        "id": "518ccdb6",
        "outputId": "81e5ebdd-7a9e-496b-dc9b-5f2c85865a25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "class\n",
              "1    4574\n",
              "0    1798\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sampled['class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f635c89",
      "metadata": {
        "id": "1f635c89",
        "outputId": "fe2f71e9-056b-41ab-c0ff-55c77f503952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class\n",
            "1    4574\n",
            "0    1798\n",
            "Name: count, dtype: int64\n",
            "class\n",
            "0    1798\n",
            "1    1798\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(df_sampled['class'].value_counts())\n",
        "\n",
        "# Separate the data into two classes\n",
        "class_0_data = df_sampled[df_sampled['class'] == 0]\n",
        "class_1_data = df_sampled[df_sampled['class'] == 1]\n",
        "\n",
        "# Downsample Class 1 to have the same number of samples as Class 0\n",
        "class_1_downsampled = class_1_data.sample(n=len(class_0_data), replace=False, random_state=42)\n",
        "\n",
        "# Concatenate the downsampled Class 1 with the original Class 0\n",
        "balanced_df = pd.concat([class_0_data, class_1_downsampled])\n",
        "\n",
        "# Display the counts of each class after downsampling\n",
        "print(balanced_df['class'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52da72be",
      "metadata": {
        "id": "52da72be",
        "outputId": "3ad1befb-49ed-4712-b64b-8ba5d13f2d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class\n",
            "0    1798\n",
            "1    1798\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Rename the balanced DataFrame to df_sampled\n",
        "df_sampled = balanced_df\n",
        "\n",
        "# Display the counts of each class in the final sampled DataFrame\n",
        "print(df_sampled['class'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "433c7180",
      "metadata": {
        "id": "433c7180"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af324a63",
      "metadata": {
        "id": "af324a63"
      },
      "outputs": [],
      "source": [
        "df_sampled1 ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d3e7ff",
      "metadata": {
        "id": "c8d3e7ff"
      },
      "outputs": [],
      "source": [
        "review_train, review_test, class_train, class_test = train_test_split(df_sampled['reviewText'],df_sampled['class'],test_size = 0.35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bc76ac0",
      "metadata": {
        "id": "9bc76ac0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef431a5",
      "metadata": {
        "id": "7ef431a5"
      },
      "outputs": [],
      "source": [
        "# Use TfidfVectorizer to convert text data into numerical vectors\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "X_train_vectorized = vectorizer.fit_transform(review_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5eb825d",
      "metadata": {
        "id": "c5eb825d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use SMOTE to oversample the minority class on training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, class_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba46f2c6",
      "metadata": {
        "id": "ba46f2c6"
      },
      "outputs": [],
      "source": [
        "df_sampled['class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aac95b3",
      "metadata": {
        "id": "1aac95b3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcca29b1",
      "metadata": {
        "id": "bcca29b1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load and split your data\n",
        "review_train, review_val, class_train, class_val = train_test_split(df_sampled['reviewText'], df_sampled['class'], test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646db1c1",
      "metadata": {
        "id": "646db1c1"
      },
      "outputs": [],
      "source": [
        "# Tokenize data\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokens_train = tokenizer(review_train.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "tokens_val = tokenizer(review_val.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b33b4f66",
      "metadata": {
        "id": "b33b4f66"
      },
      "outputs": [],
      "source": [
        "# Create DataLoader\n",
        "dataset_train = TensorDataset(tokens_train['input_ids'], tokens_train['attention_mask'], torch.tensor(class_train.tolist()))\n",
        "dataset_val = TensorDataset(tokens_val['input_ids'], tokens_val['attention_mask'], torch.tensor(class_val.tolist()))\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=16, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef95fe3f",
      "metadata": {
        "id": "ef95fe3f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec0d8f4",
      "metadata": {
        "id": "0ec0d8f4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set up optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e9b2d5",
      "metadata": {
        "id": "79e9b2d5"
      },
      "outputs": [],
      "source": [
        "# Define early stopping parameters\n",
        "patience = 3  # Number of epochs to wait for improvement\n",
        "best_val_loss = float('inf')\n",
        "current_patience = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bece6a8",
      "metadata": {
        "id": "5bece6a8"
      },
      "outputs": [],
      "source": [
        "# Training loop with early stopping\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in tqdm(dataloader_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch'):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f7bb017",
      "metadata": {
        "id": "4f7bb017"
      },
      "outputs": [],
      "source": [
        "# Validate the model on the validation set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "with torch.no_grad():\n",
        "    val_losses = []  # List to store validation losses\n",
        "    predicted_classes = []  # List to store predicted classes\n",
        "\n",
        "    # Iterate over batches in the validation dataloader\n",
        "    for val_batch in dataloader_val:\n",
        "        val_input_ids, val_attention_mask, val_labels = val_batch\n",
        "\n",
        "        # Forward pass (making predictions) without calculating gradients\n",
        "        val_outputs = model(val_input_ids, attention_mask=val_attention_mask, labels=val_labels)\n",
        "\n",
        "        # Collect validation losses\n",
        "        val_losses.append(val_outputs.loss.item())\n",
        "\n",
        "        # Extract logits from the model's output and get predicted classes\n",
        "        logits = val_outputs.logits\n",
        "        predicted_class = torch.argmax(logits, dim=1).numpy()\n",
        "        predicted_classes.extend(predicted_class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d7e1303",
      "metadata": {
        "id": "3d7e1303",
        "outputId": "eb5bd825-6754-417d-b0ee-3cbf3c56632c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3 - Validation Loss: 0.2771, Accuracy: 90.98%\n"
          ]
        }
      ],
      "source": [
        "  # Calculate validation accuracy and loss\n",
        "val_loss = sum(val_losses) / len(val_losses)\n",
        "val_accuracy = accuracy_score(class_val, predicted_classes)\n",
        "\n",
        "print(f'Epoch {epoch + 1}/{epochs} - Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2%}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1379ff9b",
      "metadata": {
        "id": "1379ff9b"
      },
      "outputs": [],
      "source": [
        "# Example loop structure\n",
        "for epoch in range(epochs):\n",
        "    # ... (previous code)\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        current_patience = 0\n",
        "    else:\n",
        "        current_patience += 1\n",
        "        if current_patience >= patience:\n",
        "            print(f'Early stopping after {epoch + 1} epochs without improvement.')\n",
        "            break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5feb5798",
      "metadata": {
        "id": "5feb5798"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('new model01')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef36319f",
      "metadata": {
        "id": "ef36319f",
        "outputId": "0d34135a-75fc-4b58-c193-7628c91d519c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 90.98%\n",
            "Validation Precision: 92.68%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "\n",
        "# Validate the model on the validation set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predicted_classes = []\n",
        "\n",
        "    for val_batch in dataloader_val:\n",
        "        val_input_ids, val_attention_mask, val_labels = val_batch\n",
        "        val_outputs = model(val_input_ids, attention_mask=val_attention_mask, labels=val_labels)\n",
        "\n",
        "        logits = val_outputs.logits\n",
        "        predicted_class = torch.argmax(logits, dim=1).numpy()\n",
        "        predicted_classes.extend(predicted_class)\n",
        "\n",
        "    # Calculate validation accuracy and precision\n",
        "    val_accuracy = accuracy_score(class_val, predicted_classes)\n",
        "    val_precision = precision_score(class_val, predicted_classes)\n",
        "\n",
        "    print(f'Validation Accuracy: {val_accuracy:.2%}')\n",
        "    print(f'Validation Precision: {val_precision:.2%}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03226198",
      "metadata": {
        "id": "03226198"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}